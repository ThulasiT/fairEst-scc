{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generate import generateSamples\n",
    "from utils.Mixture import NMixture, PUMixture\n",
    "import utils.mixtureUtils as mixtureUtils\n",
    "from utils.NestedGroupDist import NestedGroupDist\n",
    "from utils.NestedGroupDistUnknownGroup import NestedGroupDistUnknownGroup\n",
    "import utils.correct_metrics as correct_metrics\n",
    "import utils.correct_metrics_unknowngroups as correct_metrics_unknowngroups\n",
    "from utils.NNclassifier import getModel, getModelScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "comp = 2\n",
    "aucpn_range = [0.75,0.85]\n",
    "num_groups = 2\n",
    "\n",
    "total_samples = 25000  # Total number of data samples\n",
    "lam = 0.5  # Proportion of unbiased points in the total data\n",
    "\n",
    "# Both groups are equally sized\n",
    "eta = [0.5, 0.5]  # eta= [a,b] In unbiased data, a% is G1 and b% is G2  #adds up to 1\n",
    "eta_ = [0.5, 0.5]  # eta_= [a,b] In biased data, a% is G1 and b% is G2  #adds up to 1\n",
    "\n",
    "alpha = [0.5, 0.5]  # Fraction of positive unbiased samples from each group\n",
    "# 1 - alpha fraction of negative unbiased samples from each group\n",
    "alpha_ = [0.5, 0.5]  # Fraction of positive biased samples from each group\n",
    "# 1 - alpha_ negative biased samples from each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_file_name = 'synthetic_param/parameters/synthetic_datasets_params_d{}K{}.pkl'.format(dim, comp)\n",
    "params = pd.read_pickle(pkl_file_name)\n",
    "\n",
    "# Create a dataframe with these AUCs\n",
    "df = params[(params['aucpn'] > aucpn_range[0]) & (params['aucpn'] < aucpn_range[1])]\n",
    "\n",
    "i = 60\n",
    "\n",
    "# Positive distribution\n",
    "mu = {1: df['mu_p'].iloc[i],  # positive label\n",
    "        0: df['mu_n'].iloc[i]}  # negative label\n",
    "sig = {1: (df['sig_p'].iloc[i]),\n",
    "        0: (df['sig_n'].iloc[i])}\n",
    "component_weights = {0:  # group 0\n",
    "                            {1: df['w_p'].iloc[i],\n",
    "                            0: df['w_n'].iloc[i]},\n",
    "                        1:  # group 1\n",
    "                            {1: df['w_p_g2'].iloc[i],\n",
    "                            0: df['w_n_g2'].iloc[i]}}\n",
    "component_weights_labeled = {0:\n",
    "                                    {1: df['wl_p'].iloc[i],\n",
    "                                    0: df['wl_n'].iloc[i]},\n",
    "                                1:\n",
    "                                    {1: df['wl_p_g2'].iloc[i],\n",
    "                                    0: df['wl_n_g2'].iloc[i]}}\n",
    "\n",
    "# df_new = df.iloc[[60]]\n",
    "# df_new.to_csv(\"demo_param.csv\", index=False)\n",
    "# mu = {1: [[-0.21164415,  0.20583508],[ 3.48498534,  1.86657226]], # mu_p\n",
    "#       0: [[0.28298654, 1.46959782],[2.03495596, 1.76192688]]}  # mu_n\n",
    "\n",
    "# sig = {1: [[[1, 0],[0, 1]], [[1, 0],[0, 1]] ], # sig_p\n",
    "#        0: [[[1, 0],[0, 1]], [[2.19262005, 0.48831189],[0.48831189, 0.94692817]] ] } #sig_n\n",
    "\n",
    "# component_weights = {0:  # group 0\n",
    "#                             {1: df['w_p'] ,\n",
    "#                             0: df['w_n'] },\n",
    "#                         1:  # group 1\n",
    "#                             {1: df['w_p_g2'] ,\n",
    "#                             0: df['w_n_g2'] }}\n",
    "\n",
    "\n",
    "# mu = {1: df['mu_p'] ,  # positive label\n",
    "#         0: df['mu_n'] }  # negative label\n",
    "# sig = {1: (df['sig_p'] ),\n",
    "#         0: (df['sig_n'] )}\n",
    "# component_weights = {0:  # group 0\n",
    "#                             {1: [0.08674269, 0.91325731], # w_p\n",
    "#                             0: [0.18581671, 0.81418329]}, # w_n\n",
    "#                         1:  # group 1\n",
    "#                             {1: [0.74191364, 0.25808636], #w_p\n",
    "#                             0: [0.92521054, 0.07478946]} }#w_n\n",
    "\n",
    "# component_weights_labeled = {0:\n",
    "#                                     {1: df['wl_p']  ,\n",
    "#                                     0: df['wl_n'].iloc[i]},\n",
    "#                                 1:\n",
    "#                                     {1: df['wl_p_g2'].iloc[i],\n",
    "#                                     0: df['wl_n_g2'].iloc[i]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"demo_param.csv\")\n",
    "\n",
    "# mu = {1: df['mu_p'].iloc[0],  # positive label\n",
    "#         0: df['mu_n'].iloc[0]}  # negative label\n",
    "# sig = {1: (df['sig_p'].iloc[0]),\n",
    "#         0: (df['sig_n'].iloc[0])}\n",
    "\n",
    "# component_weights = {0:  # group 0\n",
    "#                             {1: df['w_p'].iloc[0],\n",
    "#                             0: df['w_n'].iloc[0]},\n",
    "#                         1:  # group 1\n",
    "#                             {1: df['w_p_g2'].iloc[0],\n",
    "#                             0: df['w_n_g2'].iloc[0]}}\n",
    "\n",
    "# component_weights_labeled = {0:\n",
    "#                                     {1: df['wl_p'].iloc[0],\n",
    "#                                     0: df['wl_n'].iloc[0]},\n",
    "#                                 1:\n",
    "#                                     {1: df['wl_p_g2'].iloc[0],\n",
    "#                                     0: df['wl_n_g2'].iloc[0]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m distrib_ub \u001b[38;5;241m=\u001b[39m {g: {label: \u001b[43mNMixture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_groups)}\n\u001b[1;32m      4\u001b[0m distrib_bias \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     g: {label: NMixture(mu[label], sig[label], np\u001b[38;5;241m.\u001b[39marray(component_weights_labeled[g][label]))\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_groups)}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Oracle distributions\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Unbiased group distributions\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Selection bias/fairScc/fairEst-scc/utils/Mixture.py:23\u001b[0m, in \u001b[0;36mNMixture.__init__\u001b[0;34m(self, mu, sigma, ps)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mps \u001b[38;5;241m=\u001b[39m ps\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         # normalize component mixture proportions if necessary\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#         if sum(ps)!=1:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#             self.ps = ps/sum(ps)            \u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_comps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# ensure correct sizes\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_comps\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "distrib_ub = {g: {label: NMixture(mu[label], sig[label], np.array(component_weights[g][label]))\n",
    "                    for label in [1, 0]} for g in range(num_groups)}\n",
    "\n",
    "distrib_bias = {\n",
    "    g: {label: NMixture(mu[label], sig[label], np.array(component_weights_labeled[g][label]))\n",
    "        for label in [1, 0]} for g in range(num_groups)}\n",
    "\n",
    "# Oracle distributions\n",
    "# Unbiased group distributions\n",
    "p_oracle = {g: PUMixture(distrib_ub[g][1], distrib_ub[g][0], alpha[g]) for g in range(num_groups)}\n",
    "\n",
    "# Biased group distributions\n",
    "q_oracle = {g: PUMixture(distrib_bias[g][1], distrib_bias[g][0], alpha_[g]) for g in range(num_groups)}\n",
    "\n",
    "# Create a distribution object for the entire unlabeled set (not conditioned on a group)\n",
    "alpha_u = {1: sum(eta[g] * alpha[g] for g in range(num_groups)),\n",
    "            0: sum(eta[g] * (1 - alpha[g]) for g in range(num_groups))}\n",
    "\n",
    "wPos_u = sum(eta[g] * alpha[g] * component_weights[g][1] for g in range(num_groups)) / alpha_u[1]\n",
    "wNeg_u = sum(eta[g] * (1 - alpha[g]) * component_weights[g][0] for g in range(num_groups)) / alpha_u[0]\n",
    "\n",
    "d_pos_ub = NMixture(mu[1], sig[1], wPos_u)\n",
    "d_neg_ub = NMixture(mu[0], sig[0], wNeg_u)\n",
    "d_ub = PUMixture(d_pos_ub, d_neg_ub, alpha_u[1])\n",
    "\n",
    "# Create a distribution object for the entire labeled set (not conditioned on a group)\n",
    "# wPos_l and wNeg_l can be estimated directly from a different fromulation of the GMM estimation, where the labeled data is treated as one sample\n",
    "# instead of two samples (one for each group).\n",
    "alpha_l = {1: sum(eta_[g] * alpha_[g] for g in range(num_groups)),\n",
    "            0: sum(eta_[g] * (1 - alpha_[g]) for g in range(num_groups))}\n",
    "wPos_l = sum(eta_[g] * alpha_[g] * component_weights_labeled[g][1] for g in range(num_groups)) / alpha_l[1]\n",
    "wNeg_l = sum(\n",
    "    eta_[g] * (1 - alpha_[g]) * component_weights_labeled[g][0] for g in range(num_groups)) / alpha_l[0]\n",
    "\n",
    "d_pos_bias = NMixture(mu[1], sig[1], wPos_l)\n",
    "d_neg_bias = NMixture(mu[0], sig[0], wNeg_l)\n",
    "d_bias = PUMixture(d_pos_bias, d_neg_bias, alpha_l[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate samples according to the base distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples according to the distributions\n",
    "s1_pos_ub, s1_neg_ub, s2_pos_ub, s2_neg_ub, s1_pos_bias, \\\n",
    "    s1_neg_bias, s2_pos_bias, s2_neg_bias \\\n",
    "    = generateSamples(distrib_ub[0][1], distrib_ub[0][0],\n",
    "                        distrib_ub[1][1], distrib_ub[1][0],\n",
    "                        distrib_bias[0][1], distrib_bias[0][0],\n",
    "                        distrib_bias[1][1], distrib_bias[1][0],\n",
    "                        total_samples, lam, alpha, alpha_, eta, eta_)\n",
    "\n",
    "# All unlabeled samples\n",
    "s_ub = np.concatenate((s1_pos_ub, s1_neg_ub, s2_pos_ub, s2_neg_ub), axis=0)\n",
    "s_pos_ub = np.concatenate([s1_pos_ub, s2_pos_ub])\n",
    "s_neg_ub = np.concatenate([s1_neg_ub, s2_neg_ub])\n",
    "s1_ub = np.concatenate((s1_pos_ub, s1_neg_ub), axis=0)\n",
    "s2_ub = np.concatenate((s2_pos_ub, s2_neg_ub), axis=0)\n",
    "\n",
    "# Positive and negative labeled samples\n",
    "s_pos_bias = np.concatenate((s1_pos_bias, s2_pos_bias), axis=0)\n",
    "s_neg_bias = np.concatenate((s1_neg_bias, s2_neg_bias), axis=0)\n",
    "# labeled data for each group\n",
    "s1_bias = np.concatenate((s1_pos_bias, s1_neg_bias), axis=0)\n",
    "s2_bias = np.concatenate((s2_pos_bias, s2_neg_bias), axis=0)\n",
    "s_all_bias = np.concatenate([s1_bias, s2_bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network classifier. Obtain scores and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the biased samples to train the model\n",
    "X = np.vstack((s_pos_bias, s_neg_bias))\n",
    "y = np.hstack((np.ones(len(s_pos_bias)), np.zeros(len(s_neg_bias))))\n",
    "\n",
    "# Get the trained model on this data\n",
    "model = getModel(X, y, threads=None)\n",
    "\n",
    "class_threshold = 0.5\n",
    "\n",
    "y_bias = {'pos': {0: getModelScores(model, s1_pos_bias),\n",
    "                    1: getModelScores(model, s2_pos_bias)},\n",
    "            'neg': {0: getModelScores(model, s1_neg_bias),\n",
    "                    1: getModelScores(model, s2_neg_bias)}}\n",
    "y_ub = {'pos': {0: getModelScores(model, s1_pos_ub),\n",
    "                1: getModelScores(model, s2_pos_ub)},\n",
    "        'neg': {0: getModelScores(model, s1_neg_ub),\n",
    "                1: getModelScores(model, s2_neg_ub)}}\n",
    "y1_all_bias = getModelScores(model, s1_bias) >= class_threshold\n",
    "y1_pos_bias = y_bias['pos'][0] >= class_threshold\n",
    "y1_neg_bias = y_bias['neg'][0] >= class_threshold\n",
    "y2_all_bias = getModelScores(model, s2_bias) >= class_threshold\n",
    "y2_pos_bias = y_bias['pos'][1] >= class_threshold\n",
    "y2_neg_bias = y_bias['neg'][1] >= class_threshold\n",
    "y_all_bias = np.concatenate([y1_all_bias, y2_all_bias])\n",
    "\n",
    "y1_all_ub = getModelScores(model, s1_ub) >= class_threshold\n",
    "y1_pos_ub = y_ub['pos'][0] >= class_threshold\n",
    "y1_neg_ub = y_ub['neg'][0] >= class_threshold\n",
    "y2_all_ub = getModelScores(model, s2_ub) >= class_threshold\n",
    "y2_pos_ub = y_ub['pos'][1] >= class_threshold\n",
    "y2_neg_ub = y_ub['neg'][1] >= class_threshold\n",
    "\n",
    "y_pos_bias = np.concatenate([y1_pos_bias, y2_pos_bias])\n",
    "y_neg_bias = np.concatenate([y1_neg_bias, y2_neg_bias])\n",
    "y_all_ub = np.concatenate([y1_all_ub, y2_all_ub])\n",
    "y_pos_ub = np.concatenate([y1_pos_ub, y2_pos_ub])\n",
    "y_neg_ub = np.concatenate([y1_neg_ub, y2_neg_ub])\n",
    "\n",
    "label_all_bias = np.concatenate([np.ones(s1_pos_bias.shape[0]), np.zeros(s1_neg_bias.shape[0]), \\\n",
    "                                    np.ones(s2_pos_bias.shape[0]), np.zeros(s2_neg_bias.shape[0])])\n",
    "label_all_bias = label_all_bias.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute TPR and FPR for groups for Correction with labelled and unlabelled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we do not have labels from the unlabeled data, we use the posterior probability from the GMM as soft labels\n",
    "YSoft_u = d_ub.pn_posterior(s_ub).reshape(-1, 1)\n",
    "# print(YSoft_u.shape)\n",
    "Y = np.concatenate([label_all_bias, YSoft_u])\n",
    "YPred = np.concatenate([y_all_bias, y_all_ub])\n",
    "WPos_l = {g: distrib_ub[g][1].points_pdf(s_all_bias)/d_pos_bias.points_pdf(s_all_bias) for g in range(num_groups)}\n",
    "WPos_u = {g: distrib_ub[g][1].points_pdf(s_ub) / d_pos_ub.points_pdf(s_ub) for g in range(num_groups)}\n",
    "TPR = {g: correct_metrics.TPR(Y, YPred, class_threshold, np.concatenate((WPos_l[g], WPos_u[g]))) for g in range(num_groups)}\n",
    "\n",
    "WNeg_l = {g: distrib_ub[g][0].points_pdf(s_all_bias) / d_neg_bias.points_pdf(s_all_bias) for g in range(num_groups)}\n",
    "WNeg_u = {g: distrib_ub[0][0].points_pdf(s_ub) / d_neg_ub.points_pdf(s_ub) for g in range(num_groups)}\n",
    "FPR = {g: correct_metrics.FPR(Y, YPred, class_threshold, np.concatenate((WNeg_l[g], WNeg_u[g]))) for g in range(num_groups)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute fairness measures (oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_measures = {measure: {\"uncorrected\": list(),\n",
    "                            \"corrected\": list(),\n",
    "                            \"corrected_l\": list()} for measure in ['eo', 'pe', 'ppv']}\n",
    "\n",
    "## Equal opportunity\n",
    "eo_group_difference, eo = correct_metrics.correct_eo(points_pos_bias={0: s1_pos_bias, 1: s2_pos_bias},\n",
    "                                                        y_pos_bias={0: y1_pos_bias, 1: y2_pos_bias},\n",
    "                                                        y_pos_ub={0: y1_pos_ub, 1: y2_pos_ub},\n",
    "                                                        d_pos_bias={g: distrib_bias[g][1] for g in\n",
    "                                                                    range(num_groups)},\n",
    "                                                        d_pos_ub={g: distrib_ub[g][1] for g in\n",
    "                                                                range(num_groups)})\n",
    "\n",
    "eo_bias_withLabelled = {\n",
    "    g: sum(y_pos_bias * distrib_ub[g][1].points_pdf(s_pos_bias) / d_pos_bias.points_pdf(s_pos_bias)) /\n",
    "        y_pos_bias.shape[0] for g in range(num_groups)}\n",
    "\n",
    "oracle_measures[\"eo\"][\"uncorrected\"].append(\n",
    "    abs(eo_group_difference['uncorrected'] - eo_group_difference['unbiased']))\n",
    "oracle_measures[\"eo\"][\"corrected\"].append(\n",
    "    abs(eo_group_difference['corrected'] - eo_group_difference['unbiased']))\n",
    "oracle_measures[\"eo\"][\"corrected_l\"].append(\n",
    "    abs((eo_bias_withLabelled[0] - eo_bias_withLabelled[1]) - eo_group_difference['unbiased']))\n",
    "\n",
    "\n",
    "## predictive equality\n",
    "pe_group_difference, pe = correct_metrics.correct_pe(points_neg_bias={0: s1_neg_bias, 1: s2_neg_bias},\n",
    "                                                        y_neg_bias={0: y1_neg_bias, 1: y2_neg_bias},\n",
    "                                                        y_neg_ub={0: y1_neg_ub, 1: y2_neg_ub},\n",
    "                                                        d_neg_bias={g: distrib_bias[g][0] for g in\n",
    "                                                                    range(num_groups)},\n",
    "                                                        d_neg_ub={g: distrib_ub[g][0] for g in\n",
    "                                                                range(num_groups)})\n",
    "\n",
    "pe_bias_withLabelled = {\n",
    "    g: sum(y_neg_bias * distrib_ub[g][0].points_pdf(s_neg_bias) / d_neg_bias.points_pdf(s_neg_bias)) /\n",
    "        y_neg_bias.shape[0] for g in range(num_groups)}\n",
    "\n",
    "\n",
    "oracle_measures[\"pe\"][\"uncorrected\"].append(\n",
    "    abs(pe_group_difference['uncorrected'] - pe_group_difference['unbiased']))\n",
    "oracle_measures[\"pe\"][\"corrected\"].append(\n",
    "    abs(pe_group_difference['corrected'] - pe_group_difference['unbiased']))\n",
    "oracle_measures[\"pe\"][\"corrected_l\"].append(\n",
    "    abs((pe_bias_withLabelled[0] - pe_bias_withLabelled[1]) - pe_group_difference['unbiased']))\n",
    "\n",
    "\n",
    "# predictive positive value = TPR*alpha / (TPR*alpha + FPR*(1-alpha))\n",
    "ppv_group_difference = correct_metrics.correct_ppv(eo, pe, alpha, alpha_)\n",
    "ppv_bias_withLabelled = {g: eo_bias_withLabelled[g] * alpha[g] / (\n",
    "            eo_bias_withLabelled[g] * alpha[g] + pe_bias_withLabelled[g] * (1 - alpha[g]))\n",
    "                            for g in range(num_groups)}\n",
    "\n",
    "\n",
    "oracle_measures[\"ppv\"][\"uncorrected\"].append(\n",
    "    abs(ppv_group_difference['uncorrected'] - ppv_group_difference['unbiased']))\n",
    "oracle_measures[\"ppv\"][\"corrected\"].append(\n",
    "    abs(ppv_group_difference['corrected'] - ppv_group_difference['unbiased']))\n",
    "oracle_measures[\"ppv\"][\"corrected_l\"].append(\n",
    "    abs((ppv_bias_withLabelled[0] - ppv_bias_withLabelled[1]) - ppv_group_difference['unbiased']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate parameters and construct estimated distributions (with group information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfit = [comp, comp]\n",
    "nested_group_EM = NestedGroupDist(x_unlabeled=s_ub,\n",
    "                                    x_labeled=[s_pos_bias, s_neg_bias],\n",
    "                                    unlabeled_groups=np.hstack([np.zeros(s1_ub.shape[0]),\n",
    "                                                                np.ones(s2_ub.shape[0])]),\n",
    "                                    labeled_groups=[np.hstack([np.zeros(s1_pos_bias.shape[0]),\n",
    "                                                                np.ones(s2_pos_bias.shape[0])]),\n",
    "                                                    np.hstack([np.zeros(s1_neg_bias.shape[0]),\n",
    "                                                                np.ones(s2_neg_bias.shape[0])])],\n",
    "                                    components=Kfit, num_classes=2, num_groups=2)\n",
    "nested_group_EM.estimate_params(max_steps=5000)\n",
    "\n",
    "alphas_1_est, alphas_2_est = nested_group_EM.alphas\n",
    "w_1_est = [nested_group_EM.w[c][0] for c in [0, 1]]  # group==0 for each class\n",
    "wl_1_est = [nested_group_EM.w_labeled[c][0] for c in [0, 1]]\n",
    "w_2_est = [nested_group_EM.w[c][1] for c in [0, 1]]  # group==1 for each class\n",
    "wl_2_est = [nested_group_EM.w_labeled[c][1] for c in [0, 1]]\n",
    "lls_1 = lls_2 = nested_group_EM.lls\n",
    "\n",
    "# Redefining mixtures with estimated parameters.\n",
    "# Distributions with estimated parameters are used for all subsequent operations\n",
    "estimated_d_unbiased = {\n",
    "    0: {\"pos\": NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], np.array(w_1_est[0])),\n",
    "        \"neg\": NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], np.array(w_1_est[1]))},\n",
    "    1: {\"pos\": NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], np.array(w_2_est[0])),\n",
    "        \"neg\": NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], np.array(w_2_est[1]))}}\n",
    "\n",
    "estimated_d_biased = {\n",
    "    0: {\"pos\": NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], np.array(wl_1_est[0])),\n",
    "        \"neg\": NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], np.array(wl_1_est[1]))},\n",
    "    1: {\"pos\": NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], np.array(wl_2_est[0])),\n",
    "        \"neg\": NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], np.array(wl_2_est[1]))}}\n",
    "\n",
    "# Redefining alpha with the estimated parameter\n",
    "alpha_est = [alphas_1_est[0], alphas_2_est[0]]\n",
    "# observed alpha\n",
    "alpha_l_est = [s1_pos_bias.shape[0] / sum([s1_pos_bias.shape[0], s1_neg_bias.shape[0]]),\n",
    "                s2_pos_bias.shape[0] / sum([s2_pos_bias.shape[0], s2_neg_bias.shape[0]])]\n",
    "\n",
    "# Unbiased group distribution\n",
    "estimated_p = {\n",
    "    0: PUMixture(estimated_d_unbiased[0]['pos'], estimated_d_unbiased[0]['neg'], alpha_est[0]),\n",
    "    1: PUMixture(estimated_d_unbiased[1]['pos'], estimated_d_unbiased[1]['neg'], alpha_est[1])}\n",
    "\n",
    "# Biased group distribution\n",
    "estimated_q = {\n",
    "    0: PUMixture(estimated_d_biased[0]['pos'], estimated_d_biased[0]['neg'], alpha_l_est[0]),\n",
    "    1: PUMixture(estimated_d_biased[1]['pos'], estimated_d_biased[1]['neg'], alpha_l_est[1])}\n",
    "\n",
    "# Create a distribution object for the entire unblabeled set (not conditioned on a group)\n",
    "est_alpha_u = {1: sum(eta[g] * alpha_est[g] for g in range(num_groups)),\n",
    "                0: sum(eta[g] * (1 - alpha_est[g]) for g in range(num_groups))}\n",
    "est_wPos_u = sum(eta[g] * alpha_est[g] * estimated_d_unbiased[g]['pos'].ps for g in\n",
    "                    range(num_groups)) / est_alpha_u[1]\n",
    "est_wNeg_u = sum(eta[g] * (1 - alpha_est[g]) * estimated_d_unbiased[g]['neg'].ps for g in\n",
    "                    range(num_groups)) / est_alpha_u[0]\n",
    "est_d_pos_ub = NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], est_wPos_u)\n",
    "est_d_neg_ub = NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], est_wNeg_u)\n",
    "est_d_ub = PUMixture(est_d_pos_ub, est_d_neg_ub, est_alpha_u[1])\n",
    "\n",
    "# Create a distribution object for the entire labeled set (not conditioned on a group)\n",
    "# wPos_l and wNeg_l can be estimated directly from a different fromulation of the GMM estimation,\n",
    "# where the labeled data is treated as one sample\n",
    "# instead of two samples (one for each group).\n",
    "est_alpha_l = {1: sum(eta_[g] * alpha_l_est[g] for g in range(num_groups)),\n",
    "                0: sum(eta_[g] * (1 - alpha_l_est[g]) for g in range(num_groups))}\n",
    "est_wPos_l = sum(\n",
    "    eta_[g] * alpha_l_est[g] * estimated_d_biased[g]['pos'].ps for g in range(num_groups)) / \\\n",
    "                est_alpha_l[1]\n",
    "est_wNeg_l = sum(eta_[g] * (1 - alpha_l_est[g]) * estimated_d_biased[g]['neg'].ps for g in\n",
    "                    range(num_groups)) / est_alpha_l[0]\n",
    "\n",
    "est_d_pos_bias = NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], est_wPos_l)\n",
    "est_d_neg_bias = NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], est_wNeg_l)\n",
    "est_d_bias = PUMixture(est_d_pos_bias, est_d_neg_bias, est_alpha_l[1])\n",
    "\n",
    "\n",
    "YSoft_u = est_d_ub.pn_posterior(s_ub).reshape(-1, 1)\n",
    "# Since we do not have labels from the unlabeled data,\n",
    "# we use the posterior probability from the GMM as soft labels\n",
    "Y = np.concatenate([label_all_bias, YSoft_u])\n",
    "YPred = np.concatenate([y_all_bias, y_all_ub])\n",
    "WPos_l = {g: estimated_d_unbiased[g][\"pos\"].points_pdf(s_all_bias) / est_d_pos_bias.points_pdf(s_all_bias)\n",
    "            for g in range(num_groups)}\n",
    "WPos_u = {g: estimated_d_unbiased[g][\"pos\"].points_pdf(s_ub) / est_d_pos_ub.points_pdf(s_ub)\n",
    "            for g in range(num_groups)}\n",
    "WNeg_l = {g: estimated_d_unbiased[g][\"neg\"].points_pdf(s_all_bias) / est_d_neg_bias.points_pdf(\n",
    "    s_all_bias) for g in range(num_groups)}\n",
    "WNeg_u = {g: estimated_d_unbiased[g][\"neg\"].points_pdf(s_ub) / est_d_neg_ub.points_pdf(s_ub)\n",
    "            for g in range(num_groups)}\n",
    "\n",
    "TPR = {g: correct_metrics.TPR(Y, YPred, class_threshold, np.concatenate((WPos_l[g], WPos_u[g])))\n",
    "        for g in range(num_groups)}\n",
    "FPR = {g: correct_metrics.FPR(Y, YPred, class_threshold, np.concatenate((WNeg_l[g], WNeg_u[g])))\n",
    "        for g in range(num_groups)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute estimated fairness metrics (with group information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_measures = {measure: {\"uncorrected\": [],\n",
    "                                \"corrected\": [],\n",
    "                                \"corrected_l\": []} for measure in ['eo', 'pe', 'ppv']}\n",
    "\n",
    "## Equal opportunity\n",
    "eo_group_difference, eo = correct_metrics.correct_eo(\n",
    "    points_pos_bias={0: s1_pos_bias, 1: s2_pos_bias},\n",
    "    y_pos_bias={0: y1_pos_bias, 1: y2_pos_bias},\n",
    "    y_pos_ub={0: y1_pos_ub, 1: y2_pos_ub},\n",
    "    d_pos_bias={0: estimated_d_biased[0][\"pos\"], 1: estimated_d_biased[1][\"pos\"]},\n",
    "    d_pos_ub={0: estimated_d_unbiased[0][\"pos\"], 1: estimated_d_unbiased[1][\"pos\"]})\n",
    "\n",
    "eo_bias_withLabelled = {\n",
    "    g: sum(\n",
    "        y_pos_bias * estimated_d_unbiased[g][\"pos\"].points_pdf(s_pos_bias) / est_d_pos_bias.points_pdf(\n",
    "        s_pos_bias)) / y_pos_bias.shape[0] for g in range(num_groups)}\n",
    "\n",
    "estimated_measures[\"eo\"][\"uncorrected\"].append(\n",
    "    abs(eo_group_difference['uncorrected'] - eo_group_difference['unbiased']))\n",
    "estimated_measures[\"eo\"][\"corrected\"].append(\n",
    "    abs(eo_group_difference['corrected'] - eo_group_difference['unbiased']))\n",
    "estimated_measures[\"eo\"][\"corrected_l\"].append(\n",
    "    abs((eo_bias_withLabelled[0] - eo_bias_withLabelled[1]) - eo_group_difference['unbiased']))\n",
    "\n",
    "\n",
    "## predictive equality\n",
    "pe_group_difference, pe = correct_metrics.correct_pe(\n",
    "    points_neg_bias={0: s1_neg_bias, 1: s2_neg_bias},\n",
    "    y_neg_bias={0: y1_neg_bias, 1: y2_neg_bias},\n",
    "    y_neg_ub={0: y1_neg_ub, 1: y2_neg_ub},\n",
    "    d_neg_bias={0: estimated_d_biased[0][\"neg\"], 1: estimated_d_biased[1][\"neg\"]},\n",
    "    d_neg_ub={0: estimated_d_unbiased[0][\"neg\"], 1: estimated_d_unbiased[1][\"neg\"]})\n",
    "pe_bias_withLabelled = {\n",
    "    g: sum(\n",
    "        y_neg_bias * estimated_d_unbiased[g][\"neg\"].points_pdf(s_neg_bias) / est_d_neg_bias.points_pdf(\n",
    "        s_neg_bias)) / y_neg_bias.shape[0] for g in range(num_groups)}\n",
    "\n",
    "\n",
    "estimated_measures[\"pe\"][\"uncorrected\"].append(\n",
    "    abs(pe_group_difference['uncorrected'] - pe_group_difference['unbiased']))\n",
    "estimated_measures[\"pe\"][\"corrected\"].append(\n",
    "    abs(pe_group_difference['corrected'] - pe_group_difference['unbiased']))\n",
    "estimated_measures[\"pe\"][\"corrected_l\"].append(\n",
    "    abs((pe_bias_withLabelled[0] - pe_bias_withLabelled[1]) - pe_group_difference['unbiased']))\n",
    "\n",
    "\n",
    "# predictive positive value = TPR*alpha / (TPR*alpha + FPR*(1-alpha))\n",
    "ppv_group_difference = correct_metrics.correct_ppv(eo, pe, alpha_est, alpha_l_est)\n",
    "ppv_bias_withLabelled = {g:  eo_bias_withLabelled[g] * alpha_est[g] / (\n",
    "            eo_bias_withLabelled[g] * alpha_est[g] + pe_bias_withLabelled[g] * (1 - alpha_est[g]))\n",
    "                            for g in range(num_groups)}\n",
    "\n",
    "\n",
    "estimated_measures[\"ppv\"][\"uncorrected\"].append(\n",
    "    abs(ppv_group_difference['uncorrected'] - ppv_group_difference['unbiased']))\n",
    "estimated_measures[\"ppv\"][\"corrected\"].append(\n",
    "    abs(ppv_group_difference['corrected'] - ppv_group_difference['unbiased']))\n",
    "estimated_measures[\"ppv\"][\"corrected_l\"].append(\n",
    "    abs((ppv_bias_withLabelled[0] - ppv_bias_withLabelled[1]) - ppv_group_difference['unbiased']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate parameters and construct estimated distributions (without group information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfit = [comp, comp]\n",
    "nested_group_EM = NestedGroupDistUnknownGroup(\n",
    "    x_unlabeled=s_ub, x_labeled=[s_pos_bias, s_neg_bias],\n",
    "    unlabeled_groups=np.hstack([np.zeros(s1_ub.shape[0]), np.ones(s2_ub.shape[0])]),\n",
    "    components=Kfit, num_classes=2, num_groups=2)\n",
    "nested_group_EM.estimate_params(max_steps=5000)\n",
    "\n",
    "alphas_1_est, alphas_2_est = nested_group_EM.alphas\n",
    "w_1_est = [nested_group_EM.w[c][0] for c in [0, 1]]  # group==0 for each class\n",
    "wl_est = [nested_group_EM.w_labeled[c] for c in [0, 1]]\n",
    "w_2_est = [nested_group_EM.w[c][1] for c in [0, 1]]  # group==1 for each class\n",
    "lls_1 = lls_2 = nested_group_EM.lls\n",
    "\n",
    "# Redefining mixtures with estimated parameters.\n",
    "# Distributions with estimated parameters are used for all subsequent operations\n",
    "estimated_d_unbiased = {\n",
    "    0: {\"pos\": NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], np.array(w_1_est[0])),\n",
    "        \"neg\": NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], np.array(w_1_est[1]))},\n",
    "    1: {\"pos\": NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], np.array(w_2_est[0])),\n",
    "        \"neg\": NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], np.array(w_2_est[1]))}}\n",
    "\n",
    "estimated_d_biased = {\n",
    "    \"pos\": NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], np.array(wl_est[0])),\n",
    "    \"neg\": NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], np.array(wl_est[1]))}\n",
    "\n",
    "# Redefining alpha with the estimated parameter\n",
    "alpha_est = [alphas_1_est[0], alphas_2_est[0]]\n",
    "# observed alpha\n",
    "alpha_l_est = s_pos_bias.shape[0] / sum([s_pos_bias.shape[0], s_neg_bias.shape[0]])\n",
    "\n",
    "# Unbiased group distribution\n",
    "estimated_p = {\n",
    "    0: PUMixture(estimated_d_unbiased[0]['pos'], estimated_d_unbiased[0]['neg'], alpha_est[0]),\n",
    "    1: PUMixture(estimated_d_unbiased[1]['pos'], estimated_d_unbiased[1]['neg'], alpha_est[1])}\n",
    "\n",
    "# Biased group distribution\n",
    "estimated_q = PUMixture(estimated_d_biased['pos'], estimated_d_biased['neg'], alpha_l_est)\n",
    "\n",
    "# Create a distribution object for the entire unlabeled set (not conditioned on a group)\n",
    "est_alpha_u = {1: sum(eta[g] * alpha_est[g] for g in range(num_groups)),\n",
    "                0: sum(eta[g] * (1 - alpha_est[g]) for g in range(num_groups))}\n",
    "est_wPos_u = sum(eta[g] * alpha_est[g] * estimated_d_unbiased[g]['pos'].ps for g in\n",
    "                    range(num_groups)) / est_alpha_u[1]\n",
    "est_wNeg_u = sum(eta[g] * (1 - alpha_est[g]) * estimated_d_unbiased[g]['neg'].ps for g in\n",
    "                    range(num_groups)) / est_alpha_u[0]\n",
    "est_d_pos_ub = NMixture(nested_group_EM.mu[0], nested_group_EM.sg[0], est_wPos_u)\n",
    "est_d_neg_ub = NMixture(nested_group_EM.mu[1], nested_group_EM.sg[1], est_wNeg_u)\n",
    "est_d_ub = PUMixture(est_d_pos_ub, est_d_neg_ub, est_alpha_u[1])\n",
    "\n",
    "# Create a distribution object for the entire labeled set (not conditioned on a group)\n",
    "# wPos_l and wNeg_l can be estimated directly from a different fromulation of the GMM estimation,\n",
    "# where the labeled data is treated as one sample\n",
    "# instead of two samples (one for each group).\n",
    "est_wPos_l = estimated_d_biased['pos'].ps\n",
    "est_wNeg_l = estimated_d_biased['neg'].ps\n",
    "\n",
    "est_d_pos_bias = estimated_d_biased['pos']\n",
    "est_d_neg_bias = estimated_d_biased['neg']\n",
    "\n",
    "YSoft_u = est_d_ub.pn_posterior(s_ub).reshape(-1, 1)\n",
    "# Since we do not have labels from the unlabeled data,\n",
    "# we use the posterior probability from the GMM as soft labels\n",
    "Y = np.concatenate([label_all_bias, YSoft_u])\n",
    "YPred = np.concatenate([y_all_bias, y_all_ub])\n",
    "WPos_l = {\n",
    "    g: estimated_d_unbiased[g][\"pos\"].points_pdf(s_all_bias) / est_d_pos_bias.points_pdf(s_all_bias)\n",
    "    for g in range(num_groups)}\n",
    "WPos_u = {g: estimated_d_unbiased[g][\"pos\"].points_pdf(s_ub) / est_d_pos_ub.points_pdf(s_ub)\n",
    "            for g in range(num_groups)}\n",
    "WNeg_l = {g: estimated_d_unbiased[g][\"neg\"].points_pdf(s_all_bias) / est_d_neg_bias.points_pdf(\n",
    "    s_all_bias) for g in range(num_groups)}\n",
    "WNeg_u = {g: estimated_d_unbiased[g][\"neg\"].points_pdf(s_ub) / est_d_neg_ub.points_pdf(s_ub)\n",
    "            for g in range(num_groups)}\n",
    "\n",
    "TPR = {g: correct_metrics.TPR(Y, YPred, class_threshold, np.concatenate((WPos_l[g], WPos_u[g])))\n",
    "        for g in range(num_groups)}\n",
    "FPR = {g: correct_metrics.FPR(Y, YPred, class_threshold, np.concatenate((WNeg_l[g], WNeg_u[g])))\n",
    "        for g in range(num_groups)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute estimated fairness metrics (without group information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_measures_ng = {measure: {\"corrected_l\": []} for measure in ['eo', 'pe', 'ppv']}\n",
    "\n",
    "## Equal opportunity\n",
    "eo_group_difference, eo = correct_metrics_unknowngroups.correct_eo(\n",
    "    y_pos_bias=y_pos_bias,\n",
    "    y_pos_ub={0: y1_pos_ub, 1: y2_pos_ub},\n",
    "    s_pos_bias=s_pos_bias,\n",
    "    q_pos=estimated_d_biased[\"pos\"],\n",
    "    p_pos={0: estimated_d_unbiased[0][\"pos\"], 1: estimated_d_unbiased[1][\"pos\"]})\n",
    "\n",
    "eo_bias_withUL = {g: TPR[g] for g in range(num_groups)}\n",
    "\n",
    "estimated_measures_ng[\"eo\"][\"corrected_l\"].append(\n",
    "    abs(eo_group_difference['corrected_l'] - eo_group_difference['unbiased']))\n",
    "\n",
    "\n",
    "## predictive equality\n",
    "pe_group_difference, pe = correct_metrics_unknowngroups.correct_pe(\n",
    "    y_neg_bias=y_neg_bias,\n",
    "    y_neg_ub={0: y1_neg_ub, 1: y2_neg_ub},\n",
    "    s_neg_bias=s_neg_bias,\n",
    "    q_neg=estimated_d_biased[\"neg\"],\n",
    "    p_neg={0: estimated_d_unbiased[0][\"neg\"], 1: estimated_d_unbiased[1][\"neg\"]})\n",
    "\n",
    "pe_bias_withUL = {g: FPR[g] for g in range(num_groups)}\n",
    "estimated_measures_ng[\"pe\"][\"corrected_l\"].append(\n",
    "    abs(pe_group_difference['corrected_l'] - pe_group_difference['unbiased']))\n",
    "\n",
    "# predictive positive value = TPR*alpha / (TPR*alpha + FPR*(1-alpha))\n",
    "ppv_group_difference = correct_metrics_unknowngroups.correct_ppv(\n",
    "    eo, pe, alpha_est)\n",
    "\n",
    "estimated_measures_ng[\"ppv\"][\"corrected_l\"].append(\n",
    "    abs(ppv_group_difference['corrected_l'] - ppv_group_difference['unbiased']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fairness Metric</th>\n",
       "      <th>Uncorrected</th>\n",
       "      <th>GIL</th>\n",
       "      <th>GIL*</th>\n",
       "      <th>GNIL1</th>\n",
       "      <th>GNIL2</th>\n",
       "      <th>GNIL*</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EO</td>\n",
       "      <td>[0.75168]</td>\n",
       "      <td>[0.0007962449439429253]</td>\n",
       "      <td>[0.0018740729152055824]</td>\n",
       "      <td>[0.008199190335749396]</td>\n",
       "      <td>[0.0098798748374051]</td>\n",
       "      <td>[0.001271630961204806]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PE</td>\n",
       "      <td>[0.76864]</td>\n",
       "      <td>[0.013467780233199278]</td>\n",
       "      <td>[0.01631650500427051]</td>\n",
       "      <td>[0.01802115222634726]</td>\n",
       "      <td>[0.013949239872702135]</td>\n",
       "      <td>[0.019744195298815548]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PPV</td>\n",
       "      <td>[0.06664404845760674]</td>\n",
       "      <td>[0.009305821623556643]</td>\n",
       "      <td>[0.008768788511785086]</td>\n",
       "      <td>[0.015158664065379135]</td>\n",
       "      <td>[0.013787523775119492]</td>\n",
       "      <td>[0.01246626847805865]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Fairness Metric            Uncorrected                      GIL  \\\n",
       "0              EO              [0.75168]  [0.0007962449439429253]   \n",
       "1              PE              [0.76864]   [0.013467780233199278]   \n",
       "2             PPV  [0.06664404845760674]   [0.009305821623556643]   \n",
       "\n",
       "                      GIL*                   GNIL1                   GNIL2  \\\n",
       "0  [0.0018740729152055824]  [0.008199190335749396]    [0.0098798748374051]   \n",
       "1    [0.01631650500427051]   [0.01802115222634726]  [0.013949239872702135]   \n",
       "2   [0.008768788511785086]  [0.015158664065379135]  [0.013787523775119492]   \n",
       "\n",
       "                    GNIL*  \n",
       "0  [0.001271630961204806]  \n",
       "1  [0.019744195298815548]  \n",
       "2   [0.01246626847805865]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"Fairness Metric\" : [\"EO\", \"PE\", \"PPV\"],\n",
    "    \"Uncorrected\" : [estimated_measures[\"eo\"][\"uncorrected\"], estimated_measures[\"pe\"][\"uncorrected\"], estimated_measures[\"ppv\"][\"uncorrected\"]],\n",
    "    \"GIL\" : [estimated_measures[\"eo\"][\"corrected\"], estimated_measures[\"pe\"][\"corrected\"], estimated_measures[\"ppv\"][\"corrected\"]],\n",
    "    \"GIL*\" : [oracle_measures[\"eo\"][\"corrected\"], oracle_measures[\"pe\"][\"corrected\"], oracle_measures[\"ppv\"][\"corrected\"]],\n",
    "    \"GNIL1\" : [estimated_measures[\"eo\"][\"corrected_l\"], estimated_measures[\"pe\"][\"corrected_l\"], estimated_measures[\"ppv\"][\"corrected_l\"]],\n",
    "    \"GNIL2\" :[estimated_measures_ng[\"eo\"][\"corrected_l\"], estimated_measures_ng[\"pe\"][\"corrected_l\"], estimated_measures_ng[\"ppv\"][\"corrected_l\"]],\n",
    "    \"GNIL*\": [oracle_measures[\"eo\"][\"corrected_l\"], oracle_measures[\"pe\"][\"corrected_l\"], oracle_measures[\"ppv\"][\"corrected_l\"]]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbfairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
